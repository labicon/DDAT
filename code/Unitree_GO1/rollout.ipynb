{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make gymnasium environment\n",
    "import gymnasium as gym\n",
    "from gymnasium.envs.mujoco import MujocoEnv\n",
    "from gymnasium import utils\n",
    "from gymnasium.spaces import Box\n",
    "\n",
    "import math\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "import numpy as np\n",
    "import os\n",
    "from typing import Optional\n",
    "import imageio\n",
    "\n",
    "DEFAULT_CAMERA_CONFIG = {\n",
    "    \"distance\": 6.0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "menagerie_path = '../../assets/Unitree_GO1' # Hi Kanghyun:)\n",
    "\n",
    "class Go1JoystickGymEnv(MujocoEnv, utils.EzPickle):\n",
    "    metadata = {\n",
    "        \"render_modes\": [\n",
    "            \"human\",\n",
    "            \"rgb_array\",\n",
    "            \"depth_array\",\n",
    "        ],\n",
    "        \"render_fps\": 50,\n",
    "    }\n",
    "    def __init__(\n",
    "        self,\n",
    "        obs_noise: float = 0.05,\n",
    "        action_scale: float = 0.3,\n",
    "        frame_skip=1,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \n",
    "        self._xml_file = os.path.join(menagerie_path, 'scene_playground.xml')\n",
    "        utils.EzPickle.__init__(\n",
    "            self,\n",
    "            self._xml_file,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        obs_shape = 48\n",
    "        observation_space = Box(low=-100., high=100., shape=(obs_shape,), dtype=np.float64)\n",
    "\n",
    "        # self._obs_noise = obs_noise\n",
    "        self._action_scale = action_scale\n",
    "\n",
    "        self._xml_dt = 0.02 # timestep from the XML file\n",
    "        MujocoEnv.__init__(\n",
    "            self,\n",
    "            self._xml_file,\n",
    "            frame_skip,\n",
    "            observation_space=observation_space,\n",
    "            default_camera_config=DEFAULT_CAMERA_CONFIG,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        assert self.model.opt.integrator == 0, 'Use Euler integration for scene_mjx_gym.xml.'\n",
    "        # assert self.frame_skip == 1, 'Use frame_skip=1 for Euler integration.'\n",
    "\n",
    "        self._init_q = np.array([0, 0, 0.27, 1, 0, 0, 0, 0.0, 0.9, -1.8, 0.0, 0.9, -1.8, 0.0, 0.9, -1.8, 0.0, 0.9, -1.8])\n",
    "        # self._init_q = np.array([0, 0, 0.27, 1, 0, 0, 0, 0.1, 0.9, -1.8, -0.1, 0.9, -1.8, 0.1, 0.9, -1.8, -0.1, 0.9, -1.8])\n",
    "        self._default_pose = np.array([0.0, 0.9, -1.8] * 4)\n",
    "        # self._default_pose = np.array([0.1, 0.9, -1.8, -0.1, 0.9, -1.8, 0.1, 0.9, -1.8, -0.1, 0.9, -1.8])\n",
    "        self.lowers = np.array([-0.7, -1.0, -2.2] * 4)\n",
    "        self.uppers = np.array([0.52, 2.1, -0.4] * 4)\n",
    "\n",
    "    def sample_command(self, command: Optional[np.ndarray] = None) -> np.ndarray:\n",
    "        lin_vel_x = [-0.6, 1.5]  # min max [m/s]\n",
    "        lin_vel_y = [-0.8, 0.8]  # min max [m/s]\n",
    "        ang_vel_yaw = [-0.7, 0.7]  # min max [rad/s]\n",
    "\n",
    "        if command is None:\n",
    "            lin_vel_x = np.random.uniform(\n",
    "                low=lin_vel_x[0], high=lin_vel_x[1]\n",
    "            )\n",
    "            lin_vel_y = np.random.uniform(\n",
    "                low=lin_vel_y[0], high=lin_vel_y[1]\n",
    "            )\n",
    "            ang_vel_yaw = np.random.uniform(\n",
    "                low=ang_vel_yaw[0], high=ang_vel_yaw[1]\n",
    "            )\n",
    "            new_cmd = np.array([lin_vel_x, lin_vel_y, ang_vel_yaw])\n",
    "        else:\n",
    "            new_cmd = command\n",
    "        \n",
    "        new_cmd = np.array([1.0, 0.0, 0.0])\n",
    "\n",
    "        return new_cmd\n",
    "    \n",
    "    def reset(self, seed=None, options=None):\n",
    "        qpos = self._init_q # + np.random.uniform(low=-0.05, high=0.05, size=self.model.nq)\n",
    "        qvel = np.zeros(self.model.nv)\n",
    "\n",
    "        self.data.qacc_warmstart[:] = 0.0\n",
    "        self.set_state(qpos, qvel)\n",
    "\n",
    "        state_info = {\n",
    "            'last_act': np.zeros(12),\n",
    "            'last_vel': np.zeros(12),\n",
    "            'command': self.sample_command(),\n",
    "            'kick': np.array([0.0, 0.0]),\n",
    "            'step': 0,\n",
    "        }\n",
    "        self.info = state_info\n",
    "\n",
    "        obs = self._get_obs(state_info)\n",
    "        self.obs = obs\n",
    "\n",
    "        return obs, state_info\n",
    "\n",
    "    def step(self, action):\n",
    "        \n",
    "        # Physics step\n",
    "        motor_targets = self._default_pose + action * self._action_scale\n",
    "        motor_targets = np.clip(motor_targets, self.lowers, self.uppers)\n",
    "        # self.data.qacc_warmstart[:] = 0.0\n",
    "        self.do_simulation(motor_targets, self.frame_skip)\n",
    "\n",
    "        # Observation data\n",
    "        obs = self._get_obs(self.info)\n",
    "        joint_angles = self.data.qpos[7:]\n",
    "        joint_vel = self.data.qvel[6:]\n",
    "        self.obs = obs\n",
    "\n",
    "        # done if joint limits are reached or robot is falling\n",
    "        done = False\n",
    "        done |= np.any(joint_angles < self.lowers)\n",
    "        done |= np.any(joint_angles > self.uppers)\n",
    "        done |= self.data.qpos[2] < 0.18\n",
    "\n",
    "        # state management\n",
    "        self.info['last_act'] = action\n",
    "\n",
    "        # Reward\n",
    "        reward = 0.0\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self.render()\n",
    "\n",
    "        return obs, reward, False, False, self.info\n",
    "\n",
    "\n",
    "    def _get_obs(self, state_info):\n",
    "        # lin_vel = self.data.sensor('local_linvel').data\n",
    "        lin_vel = self.data.qvel[:3]\n",
    "        gyro = self.data.sensor('gyro').data\n",
    "        # gyro = self.data.sensor('base_gyro').data\n",
    "        imu_site_id = 0 # imu site id\n",
    "        # print(self.data.site_xmat[imu_site_id].reshape(3, 3))\n",
    "        gravity = self.data.site_xmat[imu_site_id].reshape(3,3).T @ np.array([0, 0, -1])\n",
    "        joint_angles = self.data.qpos[7:] - self._default_pose\n",
    "        joint_vel = self.data.qvel[6:]\n",
    "        last_act = state_info['last_act']\n",
    "        command = state_info['command']\n",
    "\n",
    "        obs = np.concatenate([\n",
    "            lin_vel,\n",
    "            gyro,\n",
    "            gravity,\n",
    "            joint_angles,\n",
    "            joint_vel,\n",
    "            last_act,\n",
    "            command,\n",
    "        ])\n",
    "\n",
    "        # clip, no noise\n",
    "        obs = np.clip(obs, -100.0, 100.0) # + self._obs_noise * np.random.uniform(-1, 1, obs.shape)\n",
    "\n",
    "        previliged_obs = np.concatenate([\n",
    "            obs,\n",
    "            gyro,\n",
    "            np.zeros(3), #accelerometer\n",
    "            gravity, \n",
    "            lin_vel,\n",
    "            np.zeros(3), # angluar velocity\n",
    "            joint_angles,\n",
    "            joint_vel,\n",
    "            self.data.actuator_force,\n",
    "            np.zeros(4), # last contact\n",
    "            np.zeros(12), # feet velocity\n",
    "            np.zeros(4), # feet air time\n",
    "            np.zeros(3), # torso xfrc_applied\n",
    "            np.zeros(1), # step count\n",
    "        ])\n",
    "\n",
    "        return {\"state\": obs, \"privileged_state\": previliged_obs}\n",
    "\n",
    "    def reset_to(self, qpos, qvel):\n",
    "        self.reset()\n",
    "        # self.data.qacc_warmstart[:] = 0.0\n",
    "        self.set_state(qpos, qvel)\n",
    "\n",
    "        state_info = {\n",
    "            'last_act': np.zeros(12),\n",
    "            'last_vel': np.zeros(12),\n",
    "            'command': self.sample_command(),\n",
    "            'kick': np.array([0.0, 0.0]),\n",
    "            'step': 0,\n",
    "        }\n",
    "        self.info = state_info\n",
    "\n",
    "        obs = self._get_obs(state_info)\n",
    "        self.obs = obs\n",
    "\n",
    "        return obs\n",
    "    \n",
    "    def get_full_state(self):\n",
    "        qpos = self.data.qpos.ravel().copy()\n",
    "        qvel = self.data.qvel.ravel().copy()\n",
    "        return np.concatenate([qpos, qvel])\n",
    "    \n",
    "# Register the Gym environment\n",
    "gym.envs.register('joystick_go1', Go1JoystickGymEnv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the environment by applying zero control                                                                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environment\n",
    "env = gym.make('joystick_go1', render_mode='human')\n",
    "env = env.unwrapped\n",
    "obs, _ = env.reset()\n",
    "\n",
    "# Save obs and action history\n",
    "obs_hist = []\n",
    "act_hist = []\n",
    "\n",
    "for t in range(300):\n",
    "    act = np.zeros(12)\n",
    "    obs_hist.append(env.get_full_state())\n",
    "    act_hist.append(act)\n",
    "    obs, _, terminated, _, info = env.step(act)\n",
    "\n",
    "    # image = env.render()\n",
    "    # # save the image\n",
    "    # imageio.imwrite(f\"figure/image_{t}.png\", image)\n",
    "\n",
    "    if terminated:\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load jax policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Import MuJoCo, MJX, and Brax\n",
    "import functools\n",
    "from brax.training.agents.ppo import networks as ppo_networks\n",
    "from brax.training.agents.ppo import train as ppo\n",
    "from brax.io import model\n",
    "import jax\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from mujoco_playground import wrapper\n",
    "from mujoco_playground import registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'Go1JoystickFlatTerrain'\n",
    "env = registry.load(env_name)\n",
    "env_cfg = registry.get_default_config(env_name)\n",
    "env_cfg.Kd = 0.7\n",
    "env_cfg.Kp = 40\n",
    "\n",
    "from mujoco_playground.config import locomotion_params\n",
    "ppo_params = locomotion_params.brax_ppo_config(env_name)\n",
    "ppo_training_params = dict(ppo_params)\n",
    "network_factory = ppo_networks.make_ppo_networks\n",
    "if \"network_factory\" in ppo_params:\n",
    "  del ppo_training_params[\"network_factory\"]\n",
    "  network_factory = functools.partial(\n",
    "      ppo_networks.make_ppo_networks,\n",
    "      **ppo_params.network_factory\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fn = functools.partial(\n",
    "    ppo.train, **dict(ppo_training_params),\n",
    "    network_factory=network_factory,\n",
    ")\n",
    "restore_checkpoint_path = \"/home/kanghyunryu/mujoco_playground/checkpoints/Go1JoystickFlatTerrain/final/200540160\"\n",
    "make_inference_fn, params, metrics = train_fn(\n",
    "    environment=registry.load(env_name, config=env_cfg),\n",
    "    eval_env=registry.load(env_name, config=env_cfg),\n",
    "    wrap_env_fn=wrapper.wrap_for_brax_training,\n",
    "    # restore_checkpoint_path=restore_checkpoint_path,  # restore from the checkpoint!\n",
    "    num_timesteps=0,\n",
    ")\n",
    "\n",
    "# If you have problem with checkpoint loading, you can load the params directly\n",
    "model_path = './logs/go1_policy'\n",
    "params = model.load_params(model_path)\n",
    "\n",
    "jit_inference_fn = jax.jit(make_inference_fn(params, deterministic=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environment\n",
    "env = gym.make('joystick_go1', render_mode='human')\n",
    "env = env.unwrapped\n",
    "obs, _ = env.reset()\n",
    "env.info[\"command\"] = np.array([0.0, 0.0, 0.0])\n",
    "\n",
    "# Save obs and action history\n",
    "obs_hist = []\n",
    "act_hist = []\n",
    "\n",
    "rng = jax.random.PRNGKey(seed=1)\n",
    "\n",
    "for t in range(1000):\n",
    "    act_rng, rng = jax.random.split(rng)\n",
    "    act, _ = jit_inference_fn(obs, act_rng)\n",
    "    obs_hist.append(env.get_full_state())\n",
    "    act_hist.append(act)\n",
    "    obs, _, terminated, _, info = env.step(act)\n",
    "\n",
    "    if t == 100:\n",
    "        env.info[\"command\"] = np.array([1.0, 0.0, 0.0])\n",
    "\n",
    "    if terminated:\n",
    "        break\n",
    "\n",
    "# Save the history\n",
    "trajectory = {\n",
    "    'obs': np.array(obs_hist),\n",
    "    'act': np.array(act_hist)\n",
    "}\n",
    "\n",
    "np.save('go1_trajectory.npy', trajectory)\n",
    "np.savetxt('go1_state_trajectory.csv', np.array(obs_hist), delimiter=',')\n",
    "np.savetxt('go1_action_trajectory.csv', np.array(act_hist), delimiter=',')\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the trajectory and try to replay\n",
    "env = gym.make('joystick_go1', render_mode='human')\n",
    "env = env.unwrapped\n",
    "obs, _ = env.reset()\n",
    "\n",
    "trajectory = np.load('go1_trajectory.npy', allow_pickle=True).item()\n",
    "obs_hist = trajectory['obs']\n",
    "act_hist = trajectory['act']\n",
    "\n",
    "obs_init = obs_hist[0]\n",
    "qpos_init = obs_init[:19]\n",
    "qvel_init = obs_init[19:]\n",
    "\n",
    "for t in range(1000):\n",
    "    act = act_hist[t]\n",
    "    obs, _, terminated, _, info = env.step(act)\n",
    "\n",
    "    if terminated:\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "playground",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
